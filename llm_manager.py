"""
LLM Manager - Gestion des mod√®les de langage locaux (GGUF)
V√©rification, t√©l√©chargement et chargement automatiques
"""

import os
from pathlib import Path
from typing import Optional
from urllib.parse import urlparse
import requests
from tqdm import tqdm


class LLMManager:
    """Gestionnaire de mod√®les LLM locaux avec t√©l√©chargement automatique"""

    # Mod√®les recommand√©s avec leurs URLs Hugging Face
    RECOMMENDED_MODELS = {
        "mistral-7b-instruct-q4": {
            "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
            "filename": "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
            "size": "4.37 GB",
            "description": "Mistral 7B Instruct - Quantization Q4_K_M (recommand√©)"
        },
        "mistral-7b-instruct-q5": {
            "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf",
            "filename": "mistral-7b-instruct-v0.2.Q5_K_M.gguf",
            "size": "5.13 GB",
            "description": "Mistral 7B Instruct - Quantization Q5_K_M (meilleure qualit√©)"
        },
        "llama-3.2-3b-q4": {
            "url": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            "filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            "size": "1.95 GB",
            "description": "Llama 3.2 3B Instruct - Plus l√©ger"
        },
        "phi-3-mini-q4": {
            "url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf",
            "filename": "Phi-3-mini-4k-instruct-q4.gguf",
            "size": "2.23 GB",
            "description": "Phi-3 Mini - Tr√®s performant et l√©ger"
        }
    }

    def __init__(self, models_dir: str = "./models"):
        """
        Initialise le gestionnaire de mod√®les

        Args:
            models_dir: Dossier de stockage des mod√®les
        """
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(parents=True, exist_ok=True)

    def check_model_exists(self, model_path: str) -> bool:
        """
        V√©rifie si un mod√®le existe localement

        Args:
            model_path: Chemin vers le mod√®le

        Returns:
            True si le mod√®le existe, False sinon
        """
        path = Path(model_path)
        if path.exists() and path.is_file():
            size_mb = path.stat().st_size / (1024 * 1024)
            print(f"‚úì Mod√®le trouv√©: {model_path} ({size_mb:.2f} MB)")
            return True
        return False

    def list_available_models(self) -> list[str]:
        """Liste tous les mod√®les GGUF disponibles localement"""
        models = list(self.models_dir.glob("*.gguf"))
        return [str(m) for m in models]

    def get_recommended_models(self) -> dict:
        """Retourne la liste des mod√®les recommand√©s"""
        return self.RECOMMENDED_MODELS

    def download_model(
        self,
        url: str,
        filename: Optional[str] = None,
        chunk_size: int = 8192
    ) -> str:
        """
        T√©l√©charge un mod√®le depuis une URL

        Args:
            url: URL du mod√®le √† t√©l√©charger
            filename: Nom du fichier de destination (d√©duit de l'URL si None)
            chunk_size: Taille des chunks pour le t√©l√©chargement

        Returns:
            Chemin vers le mod√®le t√©l√©charg√©
        """
        # D√©terminer le nom de fichier
        if filename is None:
            parsed_url = urlparse(url)
            filename = Path(parsed_url.path).name

        output_path = self.models_dir / filename

        # V√©rifier si d√©j√† t√©l√©charg√©
        if output_path.exists():
            print(f"‚úì Le mod√®le existe d√©j√†: {output_path}")
            return str(output_path)

        print(f"üì• T√©l√©chargement du mod√®le: {filename}")
        print(f"   Source: {url}")

        try:
            # Requ√™te avec stream pour les gros fichiers
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()

            # Obtenir la taille totale
            total_size = int(response.headers.get('content-length', 0))

            # T√©l√©charger avec barre de progression
            with open(output_path, 'wb') as f:
                with tqdm(
                    total=total_size,
                    unit='B',
                    unit_scale=True,
                    unit_divisor=1024,
                    desc=filename
                ) as pbar:
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))

            print(f"‚úì Mod√®le t√©l√©charg√©: {output_path}")
            return str(output_path)

        except requests.exceptions.RequestException as e:
            # Nettoyer en cas d'erreur
            if output_path.exists():
                output_path.unlink()
            raise RuntimeError(f"Erreur lors du t√©l√©chargement: {e}")

    def download_recommended_model(self, model_key: str) -> str:
        """
        T√©l√©charge un mod√®le recommand√© par sa cl√©

        Args:
            model_key: Cl√© du mod√®le dans RECOMMENDED_MODELS

        Returns:
            Chemin vers le mod√®le t√©l√©charg√©
        """
        if model_key not in self.RECOMMENDED_MODELS:
            available = ", ".join(self.RECOMMENDED_MODELS.keys())
            raise ValueError(
                f"Mod√®le '{model_key}' inconnu. "
                f"Mod√®les disponibles: {available}"
            )

        model_info = self.RECOMMENDED_MODELS[model_key]
        print(f"üì¶ {model_info['description']}")
        print(f"üìä Taille: {model_info['size']}")

        return self.download_model(
            url=model_info['url'],
            filename=model_info['filename']
        )

    def get_or_download_model(
        self,
        model_path: Optional[str] = None,
        model_key: str = "mistral-7b-instruct-q4",
        auto_download: bool = True
    ) -> str:
        """
        Obtient un mod√®le (v√©rifie l'existence ou t√©l√©charge)

        Args:
            model_path: Chemin vers un mod√®le existant (prioritaire)
            model_key: Cl√© du mod√®le recommand√© √† t√©l√©charger si besoin
            auto_download: T√©l√©charger automatiquement si absent

        Returns:
            Chemin vers le mod√®le pr√™t √† utiliser
        """
        # Si un chemin est fourni, v√©rifier s'il existe
        if model_path:
            if self.check_model_exists(model_path):
                return model_path
            print(f"‚ö†Ô∏è  Mod√®le non trouv√©: {model_path}")

        # Chercher dans le dossier models
        local_models = self.list_available_models()
        if local_models:
            print(f"‚úì Mod√®les locaux trouv√©s: {len(local_models)}")
            model = local_models[0]
            print(f"   Utilisation de: {model}")
            return model

        # T√©l√©charger automatiquement si activ√©
        if auto_download:
            print("\nü§ñ Aucun mod√®le local trouv√©")
            print(f"   T√©l√©chargement automatique du mod√®le recommand√©: {model_key}")

            user_input = input("\n   Continuer? [O/n]: ").strip().lower()
            if user_input in ['n', 'non', 'no']:
                raise RuntimeError("T√©l√©chargement annul√© par l'utilisateur")

            return self.download_recommended_model(model_key)

        # Aucun mod√®le disponible
        raise FileNotFoundError(
            f"Aucun mod√®le trouv√© dans {self.models_dir}. "
            "Utilisez download_recommended_model() ou fournissez model_path."
        )

    def print_available_models(self):
        """Affiche les mod√®les disponibles localement et recommand√©s"""
        print("\n" + "="*60)
        print("üì¶ MOD√àLES LOCAUX")
        print("="*60)

        local_models = self.list_available_models()
        if local_models:
            for i, model in enumerate(local_models, 1):
                path = Path(model)
                size_mb = path.stat().st_size / (1024 * 1024)
                print(f"{i}. {path.name} ({size_mb:.2f} MB)")
        else:
            print("Aucun mod√®le local trouv√©")

        print("\n" + "="*60)
        print("üåü MOD√àLES RECOMMAND√âS")
        print("="*60)

        for key, info in self.RECOMMENDED_MODELS.items():
            print(f"\n{key}:")
            print(f"  üìù {info['description']}")
            print(f"  üìä Taille: {info['size']}")
            print(f"  üîó {info['url']}")


# Exemple d'utilisation
if __name__ == "__main__":
    manager = LLMManager()

    # Afficher les mod√®les disponibles
    manager.print_available_models()

    # Obtenir ou t√©l√©charger un mod√®le
    # model_path = manager.get_or_download_model(
    #     model_key="mistral-7b-instruct-q4",
    #     auto_download=True
    # )
    # print(f"\n‚úì Mod√®le pr√™t: {model_path}")
