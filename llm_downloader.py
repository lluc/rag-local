#!/usr/bin/env python3
"""
Module de gestion et t√©l√©chargement de mod√®les LLM
Inspir√© du syst√®me de t√©l√©chargement automatique
"""

import subprocess
import sys
import os
import shutil
from pathlib import Path
from typing import Dict, Optional

class LLMDownloader:
    """Gestionnaire de t√©l√©chargement et gestion des mod√®les LLM"""

    # Catalogue des mod√®les disponibles
    MODELS = {
        "llama-3.2-3b": {
            "url": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            "filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            "size": "~1.9GB",
            "description": "Llama 3.2 3B (Meta, RECOMMAND√â - √©quilibr√©)",
            "performance": "excellent",
            "memory": "4GB+"
        },
        "mistral-7b-instruct-q4": {
            "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
            "filename": "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
            "size": "~4.4GB",
            "description": "Mistral 7B v0.2 (Mistral AI, tr√®s performant)",
            "performance": "excellent",
            "memory": "8GB+"
        },
        "phi-3-mini": {
            "url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf",
            "filename": "phi-3-mini-4k-instruct-q4.gguf",
            "size": "~2.4GB",
            "description": "Phi-3 Mini (Microsoft, compact et rapide)",
            "performance": "bon",
            "memory": "4GB+"
        },
        "gemma-2-2b": {
            "url": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf",
            "filename": "gemma-2-2b-it-Q4_K_M.gguf",
            "size": "~1.7GB",
            "description": "Gemma 2 2B (Google, tr√®s l√©ger)",
            "performance": "bon",
            "memory": "3GB+"
        },
        "tinyllama": {
            "url": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
            "filename": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
            "size": "~700MB",
            "description": "TinyLlama 1.1B (ultra-rapide, tests)",
            "performance": "basique",
            "memory": "2GB+"
        }
    }

    def __init__(self, models_dir: str = "./models"):
        """
        Initialise le gestionnaire de mod√®les

        Args:
            models_dir: R√©pertoire de stockage des mod√®les
        """
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(exist_ok=True)

    def list_available_models(self) -> None:
        """Affiche la liste des mod√®les disponibles au t√©l√©chargement"""
        print("\nüì¶ Mod√®les LLM disponibles:")
        print("=" * 60)

        for key, model in self.MODELS.items():
            status = "‚úÖ Install√©" if self.is_model_installed(key) else "üì• Disponible"
            print(f"\nüîπ {key}")
            print(f"   {model['description']}")
            print(f"   Taille: {model['size']} | Performance: {model['performance']} | RAM: {model['memory']}")
            print(f"   Status: {status}")

    def list_installed_models(self) -> Dict[str, str]:
        """
        Liste les mod√®les install√©s localement

        Returns:
            Dict mapping model_key -> file_path
        """
        installed = {}

        for key, model in self.MODELS.items():
            filepath = self.models_dir / model["filename"]
            if filepath.exists() and filepath.stat().st_size > 1024 * 1024:  # > 1MB
                installed[key] = str(filepath)

        return installed

    def is_model_installed(self, model_key: str) -> bool:
        """V√©rifie si un mod√®le est install√©"""
        if model_key not in self.MODELS:
            return False

        filepath = self.models_dir / self.MODELS[model_key]["filename"]
        return filepath.exists() and filepath.stat().st_size > 1024 * 1024

    def get_model_path(self, model_key: str) -> Optional[str]:
        """
        R√©cup√®re le chemin d'un mod√®le install√©

        Args:
            model_key: Cl√© du mod√®le

        Returns:
            Chemin du fichier ou None si non install√©
        """
        if not self.is_model_installed(model_key):
            return None

        return str(self.models_dir / self.MODELS[model_key]["filename"])

    def check_disk_space(self, size_str: str) -> bool:
        """
        V√©rifie l'espace disque disponible

        Args:
            size_str: Taille du mod√®le (ex: "~1.9GB")

        Returns:
            True si assez d'espace disponible
        """
        try:
            # Parser la taille
            size_clean = size_str.replace('~', '').replace('GB', '').replace('MB', '')
            size_gb = float(size_clean)
            if 'MB' in size_str:
                size_gb /= 1024

            # V√©rifier l'espace libre
            statvfs = os.statvfs(self.models_dir)
            free_gb = (statvfs.f_frsize * statvfs.f_bavail) / (1024**3)

            if free_gb < size_gb + 1:  # +1GB de marge
                print(f"‚ö†Ô∏è  Espace disque faible: {free_gb:.1f}GB libre, {size_gb:.1f}GB n√©cessaires")
                return False

            return True
        except Exception:
            return True  # En cas d'erreur, on continue

    def download_with_curl(self, url: str, filepath: Path) -> bool:
        """
        T√©l√©charge un fichier avec curl

        Args:
            url: URL de t√©l√©chargement
            filepath: Chemin de destination

        Returns:
            True si succ√®s, False sinon
        """
        # V√©rifier que curl est disponible
        if not shutil.which("curl"):
            print("‚ùå curl n'est pas install√©. Installation requise:")
            print("   sudo apt install curl  # Linux")
            print("   brew install curl      # macOS")
            return False

        print(f"üì• T√©l√©chargement en cours...")
        print(f"üìÇ Destination: {filepath}")

        try:
            cmd = [
                "curl",
                "-L",                    # Suivre les redirections
                "-C", "-",              # Reprendre t√©l√©chargement si interrompu
                "-o", str(filepath),    # Fichier de sortie
                "--progress-bar",       # Barre de progression
                "--fail",               # √âchouer sur erreurs HTTP
                url
            ]

            subprocess.run(cmd, check=True)

            # V√©rifier que le fichier est valide
            if filepath.stat().st_size < 1024 * 1024:  # < 1MB
                print("‚ùå Fichier t√©l√©charg√© trop petit, probablement corrompu")
                filepath.unlink()
                return False

            print(f"\n‚úÖ T√©l√©chargement termin√©!")
            print(f"üìä Taille: {filepath.stat().st_size / (1024**2):.1f} MB")
            return True

        except subprocess.CalledProcessError as e:
            print(f"\n‚ùå Erreur de t√©l√©chargement (code {e.returncode})")
            if filepath.exists():
                filepath.unlink()
            return False

        except KeyboardInterrupt:
            print(f"\n‚ö†Ô∏è  T√©l√©chargement interrompu par l'utilisateur")
            if filepath.exists():
                filepath.unlink()
            return False

    def download_model(self, model_key: str, force: bool = False) -> bool:
        """
        T√©l√©charge un mod√®le

        Args:
            model_key: Cl√© du mod√®le √† t√©l√©charger
            force: Forcer le t√©l√©chargement m√™me si d√©j√† pr√©sent

        Returns:
            True si succ√®s, False sinon
        """
        if model_key not in self.MODELS:
            print(f"‚ùå Mod√®le '{model_key}' non trouv√© dans le catalogue")
            print(f"Mod√®les disponibles: {list(self.MODELS.keys())}")
            return False

        model = self.MODELS[model_key]
        filepath = self.models_dir / model["filename"]

        # V√©rifier si d√©j√† install√©
        if self.is_model_installed(model_key) and not force:
            print(f"‚úÖ Le mod√®le '{model_key}' est d√©j√† install√©")
            print(f"üìÇ Emplacement: {filepath}")
            return True

        # V√©rifier l'espace disque
        if not self.check_disk_space(model["size"]):
            response = input("Continuer malgr√© l'espace faible? (y/N): ").strip().lower()
            if response != 'y':
                print("T√©l√©chargement annul√©")
                return False

        print(f"\nüöÄ T√©l√©chargement de {model['description']}")
        print(f"üìä Taille: {model['size']}")

        # T√©l√©charger
        success = self.download_with_curl(model["url"], filepath)

        if success:
            print(f"\nüéâ Mod√®le '{model_key}' install√© avec succ√®s!")
            return True
        else:
            print(f"\n‚ùå √âchec du t√©l√©chargement de '{model_key}'")
            print("\nüí° Solutions alternatives:")
            print("   1. R√©essayer plus tard")
            print("   2. T√©l√©charger manuellement:")
            print(f"      URL: {model['url']}")
            print(f"      Destination: {filepath}")
            return False

    def remove_model(self, model_key: str) -> bool:
        """
        Supprime un mod√®le install√©

        Args:
            model_key: Cl√© du mod√®le √† supprimer

        Returns:
            True si succ√®s, False sinon
        """
        if not self.is_model_installed(model_key):
            print(f"‚ùå Mod√®le '{model_key}' non install√©")
            return False

        filepath = self.models_dir / self.MODELS[model_key]["filename"]

        try:
            filepath.unlink()
            print(f"‚úÖ Mod√®le '{model_key}' supprim√©")
            return True
        except Exception as e:
            print(f"‚ùå Erreur lors de la suppression: {e}")
            return False

    def get_recommended_model(self) -> str:
        """Retourne la cl√© du mod√®le recommand√©"""
        return "llama-3.2-3b"

    def auto_download_recommended(self) -> Optional[str]:
        """
        T√©l√©charge automatiquement le mod√®le recommand√© s'il n'existe aucun mod√®le

        Returns:
            Chemin du mod√®le t√©l√©charg√© ou None si √©chec
        """
        installed = self.list_installed_models()

        if installed:
            # Un mod√®le est d√©j√† install√©
            return list(installed.values())[0]

        # Aucun mod√®le install√©, t√©l√©charger le recommand√©
        recommended = self.get_recommended_model()
        print(f"üì• T√©l√©chargement automatique du mod√®le recommand√©: {recommended}")

        if self.download_model(recommended):
            return self.get_model_path(recommended)

        return None


# Exemple d'utilisation
if __name__ == "__main__":
    downloader = LLMDownloader()

    # Lister les mod√®les disponibles
    downloader.list_available_models()

    print("\n" + "="*60)

    # Lister les mod√®les install√©s
    installed = downloader.list_installed_models()
    if installed:
        print(f"\n‚úÖ Mod√®les install√©s: {len(installed)}")
        for key, path in installed.items():
            print(f"   üîπ {key}: {path}")
    else:
        print("\nüì≠ Aucun mod√®le install√©")