# RAG Local - Syst√®me RAG 100% Local

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Project Status: Active](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)

Syst√®me de Retrieval-Augmented Generation (RAG) compl√®tement local, sans d√©pendance √† des API externes. Utilise des mod√®les LLM quantifi√©s (GGUF) et ChromaDB pour la base vectorielle.

## ‚ú® Fonctionnalit√©s

- üìÑ **Traitement de documents** : PDF, Markdown, EPUB
- üóÑÔ∏è **Base vectorielle locale** : ChromaDB avec embeddings Sentence-Transformers
- ü§ñ **LLM local** : Support des mod√®les GGUF (llama.cpp)
- üì• **T√©l√©chargement automatique** : Gestion intelligente des mod√®les
- üõ°Ô∏è **100% priv√©** : Aucune donn√©e ne quitte votre machine
- üñ•Ô∏è **Interface CLI** : Commandes simples pour indexer et interroger

## üöÄ Installation

### Pr√©requis

- Python 3.11+
- uv (gestionnaire de paquets)
- curl (pour le t√©l√©chargement de mod√®les)

### Installation rapide

```bash
# Cloner le d√©p√¥t
git clone <votre-repo>
cd rag-local

# Installer les d√©pendances avec uv
uv sync
```

## üìñ Utilisation

### Interface CLI (Recommand√©e)

```bash
# Lister les mod√®les disponibles
python main.py models

# T√©l√©charger le mod√®le recommand√©
python main.py download llama-3.2-3b

# Indexer un document
python main.py index ./docs/manuel.pdf

# Indexer un dossier complet
python main.py index ./docs/

# Poser une question
python main.py query -q "R√©sume le document principal"

# Mode interactif
python main.py query --interactive

# Effacer la base de donn√©es
python main.py clear
```

### Utilisation Python

```python
from rag_local import LocalRAG

# Initialiser le RAG (Llama-3.2-3B t√©l√©charg√© automatiquement)
rag = LocalRAG(
    model_key="llama-3.2-3b",  # Mod√®le recommand√©
    auto_download=True,
    n_threads=4
)

# Indexer des documents
rag.index_document("./docs/manuel.pdf")
rag.index_document("./docs/notes.md")
rag.index_directory("./docs")  # Tout un dossier

# Poser des questions
result = rag.query("Quelle est la principale id√©e du document ?")
print(f"R√©ponse: {result['result']}")

# Voir les sources
for doc in result['source_documents']:
    print(f"Source: {doc.metadata['source']}")
```

### Avec un mod√®le d√©j√† t√©l√©charg√©

```python
rag = LocalRAG(
    model_path="./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    n_threads=4
)
```

## ü§ñ Gestion des Mod√®les

### Mod√®les disponibles

Le syst√®me propose 5 mod√®les pr√©-configur√©s :

| Mod√®le | Taille | Description | Performance |
|--------|--------|-------------|-------------|
| `llama-3.2-3b` | ~1.9GB | **Llama 3.2 3B (RECOMMAND√â)** - √âquilibr√© | Excellent |
| `mistral-7b-instruct-q4` | ~4.4GB | Mistral 7B - Tr√®s performant | Excellent |
| `phi-3-mini` | ~2.4GB | Phi-3 Mini - Compact et rapide | Bon |
| `gemma-2-2b` | ~1.7GB | Gemma 2 2B - Tr√®s l√©ger | Bon |
| `tinyllama` | ~700MB | TinyLlama - Ultra-rapide pour tests | Basique |

### CLI de gestion des mod√®les

```bash
# Voir tous les mod√®les disponibles et install√©s
python main.py models

# T√©l√©charger un mod√®le sp√©cifique
python main.py download mistral-7b-instruct-q4

# Forcer le re-t√©l√©chargement
python main.py download llama-3.2-3b --force
```

### Utilisation du LLMDownloader

```python
from llm_downloader import LLMDownloader

downloader = LLMDownloader()

# Afficher les mod√®les disponibles
downloader.list_available_models()

# T√©l√©charger un mod√®le sp√©cifique
success = downloader.download_model("llama-3.2-3b")

# Obtenir le chemin d'un mod√®le install√©
model_path = downloader.get_model_path("llama-3.2-3b")
```

## üìÅ Structure du Projet

```
rag-local/
‚îú‚îÄ‚îÄ rag_local.py              # Syst√®me RAG principal
‚îú‚îÄ‚îÄ llm_manager.py            # Gestionnaire avanc√© de mod√®les LLM
‚îú‚îÄ‚îÄ llm_downloader.py         # Module de t√©l√©chargement et catalogue
‚îú‚îÄ‚îÄ main.py                   # Interface CLI compl√®te
‚îú‚îÄ‚îÄ test_model_selection.py   # Tests de s√©lection de mod√®les
‚îú‚îÄ‚îÄ pyproject.toml            # D√©pendances et configuration
‚îú‚îÄ‚îÄ uv.lock                   # Lock file des d√©pendances
‚îú‚îÄ‚îÄ models/                   # Mod√®les GGUF (cr√©√© automatiquement)
‚îú‚îÄ‚îÄ chroma_db/               # Base vectorielle (cr√©√© automatiquement)
‚îî‚îÄ‚îÄ docs/                    # Vos documents √† indexer
```

## ‚öôÔ∏è Configuration Avanc√©e

### Personnalisation du RAG

```python
rag = LocalRAG(
    model_key="llama-3.2-3b",
    persist_directory="./ma_base_vectorielle",
    embedding_model="all-MiniLM-L6-v2",  # Mod√®le d'embeddings
    chunk_size=500,                       # Taille des chunks de texte
    chunk_overlap=100,                    # Chevauchement entre chunks
    n_ctx=4096,                          # Taille du contexte LLM
    n_threads=8,                         # Threads CPU
    retriever_k=3,                       # Nombre de documents r√©cup√©r√©s
    chain_type="stuff"                   # Strat√©gie de combinaison
)
```

### Options CLI avanc√©es

```bash
# Configuration globale
python main.py \
  --model-key llama-3.2-3b \
  --threads 8 \
  --db-path ./ma_base \
  --retriever-k 5 \
  query -q "Ma question"

# Afficher les sources dans les r√©ponses
python main.py query -q "Ma question" --show-sources
```

### Formats de documents support√©s

- **PDF** : Extraction via PyMuPDF
- **Markdown** : `.md`, `.markdown`
- **EPUB** : Livres √©lectroniques

## üß™ Tests

```bash
# Tester la s√©lection de mod√®les
python test_model_selection.py

# V√©rifier l'import des modules
python -c "from rag_local import LocalRAG; print('‚úÖ Import r√©ussi')"

# Tester l'affichage des mod√®les disponibles
python -c "from llm_downloader import LLMDownloader; LLMDownloader().list_available_models()"
```

## üîß D√©pannage

### Erreur de m√©moire lors du chargement du mod√®le

Essayez un mod√®le plus petit :
```bash
python main.py download gemma-2-2b  # ~1.7GB
python main.py download tinyllama   # ~700MB
```

### Le t√©l√©chargement √©choue

T√©l√©chargez manuellement depuis Hugging Face :

```bash
mkdir -p models
cd models
curl -L -O "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
```

### Erreurs d'imports LangChain

Le projet utilise LangChain 1.0+ avec les nouveaux packages modulaires :
- `langchain-core`
- `langchain-text-splitters`
- `langchain-community`
- `langchain-classic`
- `langchain-chroma`
- `langchain-huggingface`

### R√©ponses de mauvaise qualit√© en fran√ßais

Le syst√®me utilise d√©sormais Llama-3.2-3B par d√©faut, optimis√© pour les r√©ponses en fran√ßais. Si vous utilisez encore Mistral-7B, passez √† Llama-3.2-3B :

```bash
python main.py download llama-3.2-3b
python main.py query -q "Test en fran√ßais"
```

## üìä Performance

- **Embeddings** : ~100 documents/seconde (CPU)
- **LLM Inference** : 4-20 tokens/s selon le mod√®le et CPU
- **Base vectorielle** : ChromaDB optimis√© pour recherche locale
- **Mod√®le recommand√©** : Llama-3.2-3B (excellent compromis taille/qualit√©)

## üîí Confidentialit√©

Ce syst√®me est **100% local** :
- ‚úÖ Aucune donn√©e envoy√©e √† des API externes
- ‚úÖ Tous les calculs sur votre machine
- ‚úÖ Mod√®les stock√©s localement
- ‚úÖ Base vectorielle locale
- ‚úÖ Interface CLI sans connexion r√©seau

## üìö Ressources

- [LangChain Documentation](https://python.langchain.com/)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [ChromaDB](https://www.trychroma.com/)
- [Mod√®les GGUF](https://huggingface.co/models?search=gguf)
- [Llama 3.2](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf)

## üìù Exemples d'Usage

### Analyse de documentation technique

```bash
# Indexer votre documentation
python main.py index ./docs/technique/

# Poser des questions sp√©cifiques
python main.py query -q "Comment configurer l'authentification ?"
python main.py query -q "Quelles sont les API disponibles ?"
```

### Recherche dans des livres

```bash
# Indexer des EPUB
python main.py index ./bibliotheque/*.epub

# Mode interactif pour exploration
python main.py query --interactive
```

### Analyse de code source

```bash
# Indexer des fichiers Markdown de documentation
python main.py index ./src/docs/

# Rechercher des patterns
python main.py query -q "Comment utiliser la fonction de cache ?"
```

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou une PR.

## üìÑ Licence

MIT License

Copyright (c) 2024 RAG Local

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.